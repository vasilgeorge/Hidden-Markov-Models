{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# CO495 ASML Coursework 2 - Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this coursework, you are asked to implement filtering, smoothing, and optionally Viterbi decoding for discrete and continuous valued HMMs. Input data and initialization is provided and should be used for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below are here to guide you in your implementation of the EM and Viterbi algorithms for Hidden Markov Models. We follow Section 17.4 of _Machine Learning: A Probabilistic Perspective_ by Kevin Murphy (2012).\n",
    "\n",
    "You should write vectorized modular code to promote re-usability, efficiency, and readability.\n",
    "\n",
    "Your task is to complete the implementation and to report the results obtained from the provided initialization. You are strongly encouraged to explore different initialization schemes for the algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions and classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may use this function in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(A, dim=None, precision=1e-9):\n",
    "    \"\"\"This function is adapted from Kevin Murphy's code for Machine Learning: a Probabilistic Perspective.\n",
    "\n",
    "    Make the entries of a (multidimensional) array sum to 1\n",
    "    A, z = normalize(A) normalize the whole array, where z is the normalizing constant\n",
    "    A, z = normalize(A, dim)\n",
    "    If dim is specified, we normalize the specified dimension only.\n",
    "    dim=0 means each column sums to one\n",
    "    dim=1 means each row sums to one\n",
    "\n",
    "\n",
    "    Set any zeros to one before dividing.\n",
    "    This is valid, since s=0 iff all A(i)=0, so\n",
    "    we will get 0/1=0\n",
    "\n",
    "    Adapted from https://github.com/probml/pmtk3\"\"\"\n",
    "    \n",
    "    if dim is not None and dim > 1:\n",
    "        raise ValueError(\"Normalize doesn't support more than two dimensions.\")\n",
    "    \n",
    "    z = A.sum(dim)\n",
    "    # If z is a scalar, z.shape is an empty tuple and evaluates to False\n",
    "    if z.shape:\n",
    "        z[np.abs(z) < precision] = 1\n",
    "    elif np.abs(z) < precision:\n",
    "        return 0, 1\n",
    "    \n",
    "    if dim == 1:\n",
    "        return np.transpose(A.T / z), z\n",
    "    else:\n",
    "        return A / z, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial values are provided as namedtuples (initialization.A is the initial value for A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "InitGaussian = namedtuple('InitGaussian', ['A', 'Means', 'Variances', 'pi'])\n",
    "InitMultinomial = namedtuple('InitMultinomial', ['A', 'B', 'pi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering and Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break down your implementation according to the functions below. Feel free to create additional ones whenever you see fit, but the general flow of the algorithm should be made apparent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core of EM estimation on HMM operates on vectors of probabilities, so the main difference between EM for Gaussian HMM and multinomial HMM is the computation of the observation probabilities and which parameters to estimate.\n",
    "\n",
    "Complete the two functions below to compute the probabilities of the data for a given observation model and use them in the rest of the algorithm. Your filtering and smoothing steps should be model agnostic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "def computeSmallB_Gaussian(Y, Means, Variances, Nhidden, T, seq):\n",
    "    \"\"\"Compute the probabilities for the data points Y for a Gaussian observation model \n",
    "        with parameters Means and Variances.\n",
    "        \n",
    "        Input parameters:\n",
    "            - Y: the data\n",
    "            - Means: vector of the current estimates of the means\n",
    "            - Variances: vector of the current estimates of the variances\n",
    "            - Nhidden: number of hidden states\n",
    "            - T: length of the sequence\n",
    "        Output:\n",
    "            - b: vector of observation probabilities\n",
    "    \"\"\"\n",
    "\n",
    "    b = np.zeros((Nhidden,T))\n",
    "    for i in range(Nhidden):\n",
    "        for j in range(T):\n",
    "            b[i,j] = norm.pdf(Y[seq,j],Means[i],Variances[i])\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSmallB_Discrete(Y, B, Nhidden, T, seq):\n",
    "    \"\"\"Compute the probabilities for the data points Y for a multinomial observation model \n",
    "        with observation matrix B\n",
    "        \n",
    "        Input parameters:\n",
    "            - Y: the data\n",
    "            - B: matrix of observation probabilities\n",
    "        Output:\n",
    "            - b: vector of observation probabilities\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    b = np.zeros((Nhidden,T))\n",
    "    for i in range(Nhidden):\n",
    "        for j in range(T):\n",
    "            b[i,j] = B[i,Y[seq,j]-1]\n",
    "            \n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing and filtering: Estimation step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The E step involves smoothing and filtering. Refer to the course notes and/or to the recommended readings to implement these steps in the functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BackwardFiltering(A, b, Nhidden, T):\n",
    "    \"\"\"Perform backward filtering.\n",
    "        Input parameters:\n",
    "            - A: estimated transition matrix (between states)\n",
    "            - b: estimated observation probabilities (local evidence vector)\n",
    "            - N: number of hidden states\n",
    "            - T: length of the sequence\n",
    "        Output:\n",
    "            - beta: filtered probabilities\n",
    "    \"\"\"\n",
    "    beta = np.zeros((Nhidden,T))\n",
    "    # Initialization of the base cases\n",
    "    beta[:,-1] = np.ones(Nhidden)\n",
    "    for j in reversed(range(0,T-1)):\n",
    "        beta[:,j], Zb = normalize(np.dot(A, np.multiply(b[:,j+1],beta[:,j+1])))\n",
    "       \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ForwardFiltering(A, b, pi, Nhidden, T, j):\n",
    "    \"\"\"Filtering using the forward algorithm (Section 17.4.2 of K. Murphy's book)\n",
    "    Input:\n",
    "      - A: estimated transition matrix\n",
    "      - b: estimated observation probabilities (local evidence vector)\n",
    "      - pi: initial state distribution pi(j) = p(z_1 = j)\n",
    "    Output:\n",
    "      - Filtered belief state at time t: alpha = p(z_t|x_1:t)\n",
    "      - log p(x_1:T)\n",
    "      - Z: normalization constant\"\"\"\n",
    "    Z = np.zeros(T)\n",
    "    alpha = np.zeros((Nhidden,T))\n",
    "    (alpha[:, 0], Z[0]) = normalize(np.multiply(b[:,0],pi.T)) \n",
    "    for t in range(1,T):\n",
    "        alpha[:,t], Z[t] = normalize(np.multiply(b[:,t],np.dot(A.T,alpha[:,t-1])))\n",
    "\n",
    "    logProb = np.sum(np.log(Z))\n",
    "\n",
    "\n",
    "    return alpha, logProb, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ForwardBackwardSmoothing(A, b, pi, Nhidden, T, j):\n",
    "    \"\"\"Smoothing using the forward-backward algorithm.\n",
    "    Input:\n",
    "      - A: estimated transition matrix\n",
    "      - b: local evidence vector (observation probabilities)\n",
    "      - pi: initial distribution of states\n",
    "      - N: number of hidden states\n",
    "      - T: length of the sequence\n",
    "    Output:\n",
    "      - alpha: filtered belief state as defined in ForwardFiltering\n",
    "      - beta: conditional likelihood of future evidence as defined in BackwardFiltering\n",
    "      - gamma: gamma_t(j) proportional to alpha_t(j) * beta_t(j)\n",
    "      - lp: log probability defined in ForwardFiltering\n",
    "      - Z: constant defined in ForwardFiltering\"\"\"\n",
    "    \n",
    "    alpha, logProb, Z = ForwardFiltering(A, b, pi, Nhidden, T, j)\n",
    "\n",
    "    beta = BackwardFiltering(A, b, Nhidden, T)\n",
    "    gamma = np.zeros((Nhidden,T))\n",
    "    for t in range(0,T):\n",
    "        gamma[:,t], Zg =  normalize(np.multiply(alpha[:,t],beta[:,t])) \n",
    "    \n",
    "    return alpha, beta, gamma, logProb, Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the output of SmoothedMarginals in the maximization step for A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SmoothedMarginals(A, b, alpha, beta, T, Nhidden):\n",
    "    \"Two-sliced smoothed marginals p(z_t = i, z_t+1 = j | x_1:T)\"\n",
    "    \n",
    "    marginal = np.zeros((Nhidden, Nhidden, T-1));\n",
    "\n",
    "    for t in range(T-1):\n",
    "        marginal[:, :, t] = normalize(A * np.outer(alpha[:, t], np.transpose( (b[:, t+1] * beta[:, t+1]) ) ))[0];\n",
    "    \n",
    "    return marginal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the main algorithm in the skeletons below.\n",
    "How can you measure the performance of your model and choose an appropriate convergence criterion?\n",
    "_Hint: the logProb returned by the ForwardBackwardSmoothing function can be used_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian observation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM_estimate_gaussian(Y, Nhidden, Niter, epsilon, init):\n",
    "    \n",
    "    print(\"Estimating Gaussian...\")\n",
    "    print(\"\\n\")\n",
    "    # Dimensions of the data\n",
    "    N, T = Y.shape\n",
    "    \n",
    "    # Initialization\n",
    "    \n",
    "    # Initial transition matrix should be stochastic (rows sum to 1)\n",
    "    A = init.A\n",
    "    \n",
    "    # Initial means and variances of the emission probabilities\n",
    "    Means = init.Means\n",
    "    Variances = init.Variances;\n",
    "    \n",
    "    # Class prior\n",
    "    pi = init.pi\n",
    "    \n",
    "    ###############################################\n",
    "    # EM algorithm\n",
    "    \n",
    "    i = 0\n",
    "    # Initialize convergence criterion here\n",
    "    E_mean = np.zeros(2)\n",
    "    E_var = np.zeros(2)\n",
    "    E_ztk = np.zeros((N, Nhidden, T))\n",
    "    E_zt1 = np.zeros((N, Nhidden))\n",
    "    E_a = np.zeros((N, Nhidden, Nhidden, T-1))\n",
    "    av_log = 10\n",
    "    log_prev = epsilon \n",
    "    while i<Niter and abs(av_log - log_prev) > epsilon: # and condition on criterion and precision epsilon    \n",
    "        s_log = 0\n",
    "        for j in range(N):   # For each observation\n",
    "            # E-step\n",
    "            b = computeSmallB_Gaussian(Y, Means, Variances, Nhidden, T, j)\n",
    "            alpha, beta, gamma, logProb, Z = ForwardBackwardSmoothing(A, b, pi, Nhidden, T, j)\n",
    "            s_log += logProb\n",
    "            E_ztk[j,:,:] = gamma \n",
    "            E_zt1[j] = gamma[:,1]\n",
    "            E_a[j] = SmoothedMarginals(A, b, alpha, beta, T, Nhidden)\n",
    "\n",
    "            \n",
    "        # M-Step\n",
    "        A = np.sum(E_a, (0,3))/np.sum(E_a, (0,1,3))\n",
    "        pi = np.sum(E_zt1,0)/ np.sum(E_zt1)  \n",
    "\n",
    "        m = np.zeros(Nhidden)\n",
    "        v = np.zeros(Nhidden)\n",
    "        \n",
    "        for h in range(Nhidden):\n",
    "            for n in range(N):\n",
    "                for t in range(T):\n",
    "                    m[h] += E_ztk[n,h,t] * Y[n,t]\n",
    "                    \n",
    "        for k in range(Nhidden):\n",
    "            Means[k] = m[k] / np.sum(E_ztk[:,k,:])\n",
    "\n",
    "        for h in range(Nhidden):\n",
    "            for n in range(N):\n",
    "                for t in range(T):\n",
    "                    v[h] += E_ztk[n,h,t] * (Y[n,t] - Means[h]) * (Y[n,t] - Means[h])\n",
    "        \n",
    "        for ii in range(Nhidden):\n",
    "            Variances[ii] = v[ii] / np.sum(E_ztk[:,ii,:])\n",
    "              \n",
    "        i = i+1\n",
    "        log_prev = av_log\n",
    "        av_log = s_log/N\n",
    "    print(\"Number of iterations: \", i) \n",
    "    print(\"A:\",A)\n",
    "    print(\"pi:\", pi)\n",
    "    print(\"Means:\", Means)\n",
    "    print(\"Variances:\", Variances)    \n",
    "    return A, Means, Variances, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial observation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the maximization step for B you will have to compute a quantity involving indicators on the values of Y. One efficient way to do it is to pre-compute a representation of Y using _one-hot encoding_. In MATLAB:\n",
    "\n",
    "```% X sparse coding\n",
    "Nv = length(unique(Y));\n",
    "X = zeros(T, Nv);\n",
    "for i=1:T\n",
    "    X(i, Y(i)) = 1;\n",
    "end\n",
    "% Maximization: emission matrix\n",
    "B1 = B1 + gamma * X;```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM_estimate_multinomial(Y, Nhidden, Niter, epsilon, init):\n",
    "    \n",
    "    # Dimensions of the data\n",
    "    N, T = Y.shape\n",
    "    # Initialization\n",
    "    \n",
    "    # Initial transition matrix should be stochastic (rows sum to 1)\n",
    "    A = init.A\n",
    "    \n",
    "    # Observation matrix B\n",
    "    B = init.B\n",
    "    # Class prior\n",
    "    pi = init.pi\n",
    "    \n",
    "    ###############################################\n",
    "    # EM algorithm\n",
    "    \n",
    "    i = 0\n",
    "    # Initialize convergence criterion here\n",
    "    E_ztk = np.zeros((N, Nhidden, T))\n",
    "    E_zt1 = np.zeros((N, Nhidden))\n",
    "    E_a = np.zeros((N, Nhidden, Nhidden, T-1))\n",
    "    av_log = 10\n",
    "    log_prev = epsilon \n",
    "    while i<Niter and abs(av_log-log_prev) > epsilon: # and condition on criterion and precision epsilon\n",
    "        Nv = len(np.unique(Y))\n",
    "        B1 = np.zeros((Nhidden, Nv))\n",
    "        s_log=0\n",
    "        for j in range(N):   # For each observation\n",
    "            \n",
    "            # E-step\n",
    "            b = computeSmallB_Discrete(Y, B, Nhidden, T, j)\n",
    "            alpha, beta, gamma, logProb, Z = ForwardBackwardSmoothing(A, b, pi, Nhidden, T,j)\n",
    "            s_log += logProb\n",
    "            E_ztk[j,:,:] = gamma \n",
    "            X = np.zeros((T, Nv))\n",
    "            for k in range(T):\n",
    "                X[k, Y[j,k]-1] = 1\n",
    "            B1 = B1 + np.dot(gamma,X)\n",
    "            \n",
    "            E_zt1[j] = gamma[:,1]\n",
    "            E_a[j] = SmoothedMarginals(A, b, alpha, beta, T, Nhidden)\n",
    "            \n",
    "        # M-Step\n",
    "        A = np.sum(E_a, (0,3))/np.sum(E_a, (0,1,3))\n",
    "        pi = np.sum(E_zt1,0)/ np.sum(E_zt1)   \n",
    "        for h in range(Nhidden):\n",
    "            B[h] = B1[h]/np.sum(E_ztk[:,h,:])\n",
    "       # print(\"New B is: \", B)\n",
    "        i += 1\n",
    "        log_prev = av_log\n",
    "        av_log = s_log/N\n",
    "    print(\"Number of iterations: \", i)    \n",
    "    print(\"B is:\", B)\n",
    "    print(\"pi is: \", pi)\n",
    "    print(\"A is:\", A)\n",
    "    return A, B, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viterbi decoding should be performed on the smoothed data and most of the algorithm doesn't depend on the output model. To help you, we identified the steps that are model specific. Implement Viterbi decoding by completing the skeleton below. 'smallB' is a function and should be used in the standard way: smallB(x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ViterbiDecode(Y, Nhidden, outModel, init):\n",
    "    \n",
    "    if outModel == 'gauss':\n",
    "        A, Mu, Sigma, Pi = EM_estimate_gaussian(Y, Nhidden, 100, 1e-6, init)\n",
    "        smallB = lambda X : computeSmallB_Gaussian(X, Mu, Sigma, Nhidden, len(X))\n",
    "    elif outModel == 'multinomial':\n",
    "        A, B, Pi = EM_estimate_multinomial(Y, Nhidden, 100, 1e-6, init)\n",
    "        smallB = lambda X : computeSmallB_Discrete(X, B)\n",
    "    else:\n",
    "        raise ValueError('Invalid observation model: must be either \"gauss\" or \"multinomial\"')\n",
    "        \n",
    "    # Implement Viterbi decoding here.\n",
    "    \n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating Gaussian...\n",
      "Initialised Means:  [[0.93504614]\n",
      " [5.60985797]]\n",
      "Initialised Variances:  [[0.54688152]\n",
      " [0.95750684]]\n",
      "Number of iterations:  51\n",
      "A: [[0.38832547 0.38608991]\n",
      " [0.61167453 0.61391009]]\n",
      "pi: [0.33392947 0.66607053]\n",
      "Means: [[0.06975463]\n",
      " [4.97302249]]\n",
      "Variances: [[0.37220458]\n",
      " [0.91312145]]\n",
      "Estimating Multinomial\n",
      "B is:  [[0.25796198 0.32504732 0.04640567 0.03135054 0.17721777 0.16201672]\n",
      " [0.25904333 0.05332419 0.19334562 0.14589911 0.23912919 0.10925857]]\n",
      "Number of iterations:  100\n",
      "B is: [[0.10979842 0.30528693 0.03692295 0.02734931 0.0801274  0.44051499]\n",
      " [0.13567859 0.0709345  0.15867929 0.15413939 0.14505279 0.33551544]]\n",
      "pi is:  [0.23097902 0.76902098]\n",
      "A is: [[0.26247072 0.24292709]\n",
      " [0.73752928 0.75707291]]\n"
     ]
    }
   ],
   "source": [
    "with np.load('init_gaussian.npz') as f:\n",
    "    init_g = InitGaussian(f['arr_0'], f['arr_1'], f['arr_2'], f['arr_3'])\n",
    "\n",
    "with np.load('init_multinomial.npz') as f:\n",
    "    init_m = InitMultinomial(f['arr_0'], f['arr_1'], f['arr_2'])\n",
    "\n",
    "with np.load('data_gaussian.npz') as f:\n",
    "    Y_c, S_c = f['arr_0'], f['arr_1']\n",
    "\n",
    "\n",
    "with np.load('data_multinomial.npz') as f:\n",
    "    Y_d, S_d = f['arr_0'], f['arr_1']\n",
    "\n",
    "    \n",
    "A_g, Means_g, Variances_g, Pi_g = EM_estimate_gaussian(Y_c, 2, 100, 1e-6, init_g)\n",
    "print(\"\\n\")\n",
    "print(\"Estimating Multinomial\")\n",
    "print(\"\\n\")\n",
    "A_m, B_m, Pi_m = EM_estimate_multinomial(Y_d, 2, 100, 1e-6, init_m)\n",
    "\n",
    "# S_g = ViterbiDecode(Y_c, 2, 'gauss', init_g)\n",
    "# S_m = ViterbiDecode(Y_d, 2, 'multinomial', init_m)\n",
    "\n",
    "#print('*** Viterbi decoding accuracy (Gaussian): {}'.format( (S_c == S_g).sum() / S_c.size ))\n",
    "#print('*** Viterbi decoding accuracy (Multinomial): {}'.format( (S_d == S_m).sum() / S_d.size ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
